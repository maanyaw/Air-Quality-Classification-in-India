# -*- coding: utf-8 -*-
"""Air Quality Classification in India .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--NM6aUMLkazvR30MRoVY_KeBcvitsg2
"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import warnings

warnings.filterwarnings("ignore")

df = pd.read_csv("IndianWeatherRepository.csv")

selected_columns = df[['country', 'location_name', 'region','air_quality_Carbon_Monoxide','air_quality_Ozone','air_quality_Nitrogen_dioxide','air_quality_Sulphur_dioxide','air_quality_PM2.5','air_quality_PM10','air_quality_us-epa-index','air_quality_gb-defra-index']]


print(selected_columns)



#Code to Handle Missing Values
# Check for missing values
print(selected_columns.isnull().sum())

#Removing Duplicates
# Check for duplicates
print(f"Duplicate rows: {selected_columns.duplicated().sum()}")

# Remove duplicate rows
selected_columns_cleaned = selected_columns.drop_duplicates()

print(f"Duplicate rows after cleaning: {selected_columns_cleaned.duplicated().sum()}")

#Handling Outliers
# Select only numerical columns for outlier detection
numerical_columns = selected_columns_cleaned.select_dtypes(include=['number'])

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = numerical_columns.quantile(0.25)
Q3 = numerical_columns.quantile(0.75)
IQR = Q3 - Q1

# Identify outliers by filtering values outside of 1.5 * IQR from Q1 and Q3
outliers = ((numerical_columns < (Q1 - 1.5 * IQR)) | (numerical_columns > (Q3 + 1.5 * IQR)))

# Remove rows with any outliers from the original DataFrame
# Use the index of the outliers identified in the numerical columns
selected_columns_cleaned_no_outliers = selected_columns_cleaned[~outliers.any(axis=1)]

print(f"Number of rows before outlier removal: {selected_columns_cleaned.shape[0]}")
print(f"Number of rows after outlier removal: {selected_columns_cleaned_no_outliers.shape[0]}")

# Visualizing the Data After Cleaning
import plotly.express as px

# List of air quality columns to visualize
air_quality_columns = [
    'air_quality_Carbon_Monoxide', 'air_quality_Ozone', 'air_quality_Nitrogen_dioxide',
    'air_quality_Sulphur_dioxide', 'air_quality_PM2.5', 'air_quality_PM10'
]

# Visualize before outlier removal for each air quality factor
for col in air_quality_columns:
    fig_before = px.box(selected_columns_cleaned, y=col, title=f'{col} Boxplot Before Outlier Removal')
    fig_before.show()

# Visualize after outlier removal for each air quality factor
for col in air_quality_columns:
    fig_after = px.box(selected_columns_cleaned_no_outliers, y=col, title=f'{col} Boxplot After Outlier Removal')
    fig_after.show()



#Code for Feature Selection

selected_features = selected_columns_cleaned.drop(['country'], axis=1)

selected_features = selected_columns_cleaned[['air_quality_Carbon_Monoxide', 'air_quality_Ozone',
                                              'air_quality_Nitrogen_dioxide', 'air_quality_Sulphur_dioxide',
                                              'air_quality_PM2.5', 'air_quality_PM10',
                                              'air_quality_us-epa-index', 'air_quality_gb-defra-index']]

print(selected_features.head())

#Code for Min-Max Normalization
from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Select the pollutants for scaling
pollutants = ['air_quality_Carbon_Monoxide', 'air_quality_Ozone',
              'air_quality_Nitrogen_dioxide', 'air_quality_Sulphur_dioxide',
              'air_quality_PM2.5', 'air_quality_PM10']

# Normalize the data (between 0 and 1)
selected_features[pollutants] = scaler.fit_transform(selected_features[pollutants])

print(selected_features.head())

# Categorizing based on air_quality_us-epa-index
def categorize_us_epa(aqi_value):
    if aqi_value == 1:
        return 'Good'
    elif aqi_value == 2:
        return 'Moderate'
    elif aqi_value == 3:
        return 'Unhealthy for Sensitive Groups'
    elif aqi_value == 4:
        return 'Unhealthy'
    else:
        return 'Hazardous'

# Apply to the 'air_quality_us-epa-index' column
selected_features['us_epa_aqi_category'] = selected_features['air_quality_us-epa-index'].apply(categorize_us_epa)

# Check the result
print(selected_features[['air_quality_us-epa-index', 'us_epa_aqi_category']].head())

#EDA
#Histogram
import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure size for visualizations
plt.figure(figsize=(15, 10))

# Plot histograms for all relevant pollutants
pollutants = ['air_quality_Carbon_Monoxide', 'air_quality_Ozone', 'air_quality_Nitrogen_dioxide',
              'air_quality_Sulphur_dioxide', 'air_quality_PM2.5', 'air_quality_PM10']

preprocessed_data = selected_features

for i, pollutant in enumerate(pollutants, 1):
    plt.subplot(2, 3, i)
    sns.histplot(preprocessed_data[pollutant], bins=30, kde=True)
    plt.title(f"Distribution of {pollutant}")
    plt.xlabel(pollutant)
    plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# Correlation matrix between pollutants and AQI categories
correlation_matrix = preprocessed_data[['air_quality_Carbon_Monoxide', 'air_quality_Ozone',
                                        'air_quality_Nitrogen_dioxide', 'air_quality_Sulphur_dioxide',
                                        'air_quality_PM2.5', 'air_quality_PM10', 'air_quality_us-epa-index']].corr()

# Plot the correlation heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap Between Pollutants and AQI")
plt.show()

#Code for checking class distribution
#class distribution after preprocessing (e.g., after encoding the AQI categories)
class_counts = preprocessed_data['us_epa_aqi_category'].value_counts()

# Plot class distribution using a bar plot
plt.figure(figsize=(8, 6))
sns.countplot(x='us_epa_aqi_category', data=preprocessed_data, palette="Set2")
plt.title("Distribution of AQI Categories")
plt.xlabel("AQI Category")
plt.ylabel("Count")
plt.show()

# Display class distribution count
print("Class Distribution:")
print(class_counts)

# custom mapping for AQI categories
category_mapping = {
    'Good': 0,
    'Moderate': 1,
    'Unhealthy for Sensitive Groups': 2,
    'Unhealthy': 3,
    'Hazardous': 4
}

# Apply the manual mapping to the 'us_epa_aqi_category' column
selected_features['us_epa_aqi_category_encoded'] = selected_features['us_epa_aqi_category'].map(category_mapping)

# Check the encoded values again
print(selected_features[['us_epa_aqi_category', 'us_epa_aqi_category_encoded']].head())

print(df.head())

from sklearn.model_selection import train_test_split

# Define your features (X) and target (y)
X = preprocessed_data.drop(columns=['us_epa_aqi_category'])  # Drop target column
y = preprocessed_data['us_epa_aqi_category']  # Your target column

# Split the data into training and testing sets (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#Tuning Decision Tree
# Define hyperparameter grid for Decision Tree
param_grid_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_grid_dt, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)

# Fit the model
grid_search_dt.fit(X_train, y_train_encoded)

# Best parameters and accuracy
print("Best Parameters for Decision Tree:", grid_search_dt.best_params_)
print("Best Accuracy for Decision Tree:", grid_search_dt.best_score_)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Initialize the label encoder
label_encoder = LabelEncoder()

# Fit the label encoder on the target variable in the training data
# This step ensures that the encoder learns the unique classes
label_encoder.fit(y_train)

# label encoding for the target variable in both training and testing sets
y_train_encoded = label_encoder.transform(y_train)
y_test_encoded = label_encoder.transform(y_test)


# Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)

# train the model using the encoded target labels
dt_model.fit(X_train, y_train_encoded)  # Fit the model

#Evaluate Model Performance on Training Set
from sklearn.metrics import accuracy_score

# Make predictions on the training set
y_train_pred = dt_model.predict(X_train)

# Evaluate the accuracy of the model
train_accuracy = accuracy_score(y_train_encoded, y_train_pred)
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")

#Hyperparameter Tuning
#Grid Search for Random Forest
from sklearn.model_selection import GridSearchCV

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)

# Fit the model with the best parameters
grid_search.fit(X_train, y_train_encoded)

# Best parameters and best accuracy
print("Best Parameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

#Random Forest
from sklearn.ensemble import RandomForestClassifier

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Train the Random Forest model
rf_model.fit(X_train, y_train_encoded)

# Evaluate the model
y_train_pred_rf = rf_model.predict(X_train)
print(f"Random Forest Test Accuracy: {accuracy_score(y_train_encoded, y_train_pred_rf) * 100:.2f}%")

#Compare Model Performance
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, accuracy_score

# classification report for Decision Tree
print("Decision Tree Classification Report:")
print(classification_report(y_test_encoded, dt_model.predict(X_test)))

#classification report for Random Forest
print("Random Forest Classification Report:")
print(classification_report(y_test_encoded, rf_model.predict(X_test)))

# Accuracy comparison
dt_accuracy = accuracy_score(y_test_encoded, dt_model.predict(X_test))
rf_accuracy = accuracy_score(y_test_encoded, rf_model.predict(X_test))
print(f"Decision Tree Accuracy: {dt_accuracy * 100:.2f}%")
print(f"Random Forest Accuracy: {rf_accuracy * 100:.2f}%")

#Confusion Matrix Analysis
from sklearn.metrics import ConfusionMatrixDisplay

# Confusion Matrix for Decision Tree
ConfusionMatrixDisplay.from_estimator(dt_model, X_test, y_test_encoded, display_labels=label_encoder.classes_, cmap='Blues')

# Confusion Matrix for Random Forest
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test_encoded, display_labels=label_encoder.classes_, cmap='Greens')

print("Training features (X_train):", X_train.shape)
print("Columns in X_train:", X_train.columns)

# Example of new data (without location_name)
new_data = pd.DataFrame({
    'air_quality_Carbon_Monoxide': [0.3, 0.1],
    'air_quality_Ozone': [0.04, 0.03],
    'air_quality_Nitrogen_dioxide': [0.02, 0.01],
    'air_quality_Sulphur_dioxide': [0.01, 0.02],
    'air_quality_PM2.5': [0.05, 0.07],
    'air_quality_PM10': [0.08, 0.1],
    'air_quality_us-epa-index': [1, 2],  # New data for prediction
    'air_quality_gb-defra-index': [1, 2]  # Use original column name from X_train
})

# 1. Encode 'us_epa_aqi_category_encoded' based on 'air_quality_us-epa-index'
def categorize_us_epa(aqi_value):
    if aqi_value == 1:
        return 'Good'
    elif aqi_value == 2:
        return 'Moderate'
    elif aqi_value == 3:
        return 'Unhealthy for Sensitive Groups'
    elif aqi_value == 4:
        return 'Unhealthy'
    else:
        return 'Hazardous'

new_data['us_epa_aqi_category'] = new_data['air_quality_us-epa-index'].apply(categorize_us_epa)

# Encoding the 'us_epa_aqi_category' column the same way as during training
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
# Fit the encoder to the original categories (assumes the training encoder is used)
new_data['us_epa_aqi_category_encoded'] = label_encoder.fit_transform(new_data['us_epa_aqi_category'])

# Now check the new data to ensure correct encoding
print(new_data[['air_quality_us-epa-index', 'us_epa_aqi_category']])

# 2. Drop 'us_epa_aqi_category' as it's no longer needed
new_data_encoded = new_data.drop(columns=['us_epa_aqi_category'])  # Remove unnecessary column

# 3. Ensure the column order matches exactly as in the training data (X_train.columns)
# Select columns that match X_train
new_data_encoded = new_data_encoded[X_train.columns]

# 4. Normalize or scale using the same scaler used for training
scaler = MinMaxScaler()
scaler.fit(X_train)

new_data_scaled = scaler.transform(new_data_encoded)  # Apply scaling



# Make predictions on the new data
dt_predictions = dt_model.predict(new_data_scaled)
rf_predictions = rf_model.predict(new_data_scaled)

# Decode predictions back to categories using the reverse mapping
dt_predictions_labels = [reverse_category_mapping[pred] for pred in dt_predictions]
rf_predictions_labels = [reverse_category_mapping[pred] for pred in rf_predictions]

# Print the predictions
print("Decision Tree Predictions:", dt_predictions_labels)
print("Random Forest Predictions:", rf_predictions_labels)